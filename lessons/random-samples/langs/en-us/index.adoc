= Randomness and Sample Size

@description{Students explore sampling and probability as a mechanism for detecting patterns. After exploring this in a binary system (flipping a coin), they consider the role of sampling as it applies to relationships in a dataset.}

@lesson-prereqs{displaying-categorical-data}

@keywords{probability, sampling, p-value, null hypothesis,}

@add-to-lang{random-rows}

[@lesson-intro-table]
|===

| Lesson Goals
| Students will be able to...

* Take random samples from a population
* Understand the need for random samples
* Understand the role of sample size

| Student-facing Lesson Goals
|

* Let's explore how random sampling can be used with datasets.

| Materials
|[.materials-links]
@ifproglang{pyret}{
* @link{https://docs.google.com/presentation/d/1ZRxZgSJssoLzHe_a59Qw49mip0pwzcM-wVHqWTS8jTk/, Lesson Slides}
}
@ifproglang{codap}{
* @link{https://docs.google.com/presentation/d/1-r7mOckg3eHfpu4HueXl4yHxdwJmrN5B1ZXm4pLul48/edit?usp=sharing, Lesson Slides}
}

@material-links
|===

== How to Spot a Cheater @duration{10 minutes}

=== Overview
Students consider a classic randomness scenario: the probability that a coin will land on heads or tails. From a data science perspective, this can be flipped from a discussion of _probability_ to one of _inference_. Specifically, "how many samples do we need, to determine whether a coin is fair or not?"

=== Launch

@right{@image{images/heads.png, 110, "A coin, showing the heads side"}} A stranger on the street invites you to a game of chance. They'll flip a coin, and you'll win money if you can predict whether it lands heads-or-tails. If you guess wrong, however, you pay THEM. "It's a pure game of chance", they tell you, "we each have equal odds of winning".

[.lesson-instruction]
- What do you think? Can you trust them to play fair?
- For a fair coin, what are the chances of it landing heads? Tails?
- How do you know if a coin is fair or not?

=== Investigate
[.lesson-point]
A fair coin should land on "heads" about as often as it lands on "tails"

When we approach a strange coin, we start out assuming it's fair. This is called the @vocab{null hypothesis}, which means we assuming it's a fair coin and that it should land on "heads" about 50% of the time. A weighted coin, on the other hand, might be heavier on one side to force it to come up heads more often! But how do we test whether a coin is fair or not? _How do we test the null hypothesis?_

[.lesson-instruction]
Open @starter-file{fair-coins}, and complete @printable-exercise{fair-coins.adoc}

Have students share back their sample results, and their predictions after 6 samples and then 20 samples.

Which samples seem to support the null hypothesis? Which ones undermine it? In Statistics and Data Science, samples like these do not *prove* any claim about the coins! Instead, they either _produce evidence for or against_ a claim.

The larger the sample, the more evidence we have to support or reject the @vocab{null hypothesis}. The chances of getting tails from a fair coin three times in a row are pretty good! Maybe it was just the luck of the draw, but the coin is still fair. But 10 times in a row? 100 times? We might say "there's a one in a million chance of a fair coin coming up tails 100 times, so I don't trust the coin!"

Statisticians use slightly more fomal language for this. In fact, they even have a special name for the chance of a null hypothesis, called the *p-value*. A Data Scientist or Statistician would say "this sample suggests the coin is weighted (p<0.000001)", meaning "the chances of a fair coin turning up heads 100 times in a row is one-in-on-million"

=== Common Misconceptions
Students may think that _any_ sample from a fair coin should have an equal number of heads and tails outcomes. That's not true at all! A fair coin _might_ land on "tails" three times in row! The fact that this is possible doesn't mean it's _likely_. Landing on "tails" five times in row? Still possible, but much less likely.

This is where arithmetic thinking and statistical thinking diverge: it's not a question of what is _possible_, but rather what is _probable_.

=== Synthesis
- Could a coin come up "heads" twice in a row, and still be a fair coin? Why or why not?
- What about 10 times in a row? 20?
- What is the relationship between how weighted a coin is, and how many samples you need to figure it out?
- Suppose we are rolling a 6-sided die. How could we tell if it's weighted or not?

A fair coin should come up heads about 50% of the time. If a coin came up heads 100% of the time, it wouldn't take long to figure out what something was up! But a coin that comes up heads just 60% of the time would need a much larger sample to be detected. There's a connection to be found here, between _sample size_ and _deviation_.  A subtle deviation might be enough to guarantee that a casino turn a profit, and be virtually undetectable without a massive sample!

== Flip the Script: Inference v. Probability @duration{30 minutes}

=== Overview
Statistical inference involves looking at a sample and trying to _infer something you don’t know_ about a larger population. This requires a sort of backwards reasoning, kind of like making a guess about a _cause_, based on the _effect_ that we see.

=== Launch
[.lesson-point]
Inference Reasons Backwards; Probability Reasons Forwards

In the coin-flip activity, you didn't actually know what the true weighting of each coin was. But with a large enough sample, you were able to _infer_ what the real weighting of the coins might be, and reason about what would happen for additional samples.

One of the most useful tasks in Data Science is using sample data to _infer_ (guess) what’s true about the larger population from which the sample was taken. This process, called @vocab{statistical inference}, is used to gain information in practically every field of study you can imagine: medicine, business, politics, history; even art!

Suppose we want to estimate what percentage of all Americans plan to vote for a certain candidate. We can't ask everyone who they’re voting for, so pollsters instead take a _sample_ of Americans, and _generalize_ the opinion of the sample to estimate how Americans as a whole feel.

[.lesson-point]
A split electorate votes about as much for one candidate as the other

Just like our coin-flip, we can start out assuming "the coin is fair" and the vote is split equally. Flipping a coin 10 times isn't enough to prove that it's weighted, and polling 10 people isn't enough to prove that one candidate is in the lead. _Sample size matters!_

But choosing a sample can be tricky...

[.lesson-instruction]
* Would it be problematic to only call voters who are registered Democrats? To only call voters under 25? To only call regular churchgoers? Why or why not?
* How could we choose a representative subset, or _sample_ of American voters?
* Would it be problematic to only sample a handful of voters? What do we gain by taking a larger sample?

[.lesson-point]
Before we infer something _unknown_ about a population from a sample, we need to know what makes a "good" sample!

Sampling is a complicated issue. The main reason for doing inference is to guess about something that’s _unknown_ for the whole population. But a useful step along the way is to practice with situations where we happen to _know_ what’s true for the whole population. As an exercise, we can keep taking random samples from that population and see how close they tend to get us to the truth. Another discovery (besides the value of randomness) that statisticians made early on was something that’s consistent with our coin-flip example: Larger samples are better than smaller ones, because they tend to get us closer to the truth about the whole population.

Let’s see what happens if we switch from smaller to larger sample sizes, if we’re taking a random sample of shelter animals to infer what’s true about the larger population...

[.lesson-instruction]
Students should open @starter-file{expanded-animals} and save a copy.

=== Investigate

The Animals Dataset we've been using is just one _sample_ taken from a very large animal shelter. @ifproglang{pyret}{How much can we infer about the whole population of hundreds of animals, by looking at just this one sample?

[.lesson-instruction]
- Divide the class into groups of 3-5 students.
- Have students open the @starter-file{expanded-animals}, save a copy and click "Run".
- Have students complete @printable-exercise{pages/sampling-and-inference.adoc}, sharing their results and discussing with the group.
- For a deeper exploration of the impact of sample size, have students complete @opt-printable-exercise{pages/predictions-from-samples.adoc}
}

@ifproglang{codap}{We're going to analyze which is better at guessing the truth about an entire population - a small sample of 10 randomly selected animals, or a large sample of 40 randomly selected animals.

[.lesson-instruction]
Select `Sampler` from the Plugins dropdown menu.

@ifproglang{codap}{@centered-image{images/sampler-plugin-default.PNG, Sample plugin default,250}}

The `Sampler` plugin features a _Mixer_, _Spinner_, and _Collector_. Today, we’ll be using the _Collector_, which chooses a specified number of cases from a dataset.

[.lesson-instruction]
What do you _notice_ about the `Sampler`? What do you _wonder_?

Possible wonderings include: How many turquoise balls are there? Why is there that amount? How many brackets are alongside the collection of turquoise balls? Why are there that many?

[.lesson-instruction]
- Select the `Options` tab of the `Sampler`.
- Which makes the most sense for our dataset: collecting cases _with replacement_ or _without replacement_?

Note: If a particular animal can be selected more than one time, then we are sampling _with replacement_.  In a drawing-names-from-a-hat scenario, we’d return each name to the hat after selecting it. If a particular animal can be selected only one time, then we are sampling _without replacement_. In a drawing-names-from-a-hat scenario, we’d remove each name from the hat after selecting it.

[.lesson-instruction]
- Designate the number of items to select and the number of samples to collect.
- What would it mean to select three samples of five items each? (These are CODAP's default settings.)
- Enter the correct specifications for 1 collection of 10 items.
-  Click `Start` to observe the sampling simulation.

After the simulation is complete, a hierarchical table (titled `experiment/samples/items`) will be populated. Ensure that students understand all the components of the new table they’ve created.

[.lesson-instruction]
- Rename the table (by clicking on its title) `small-sample`.

Now that students are comfortable using the `Sampler`, it's time to dig into the data.

[.lesson-instruction]
- Divide the class into groups of 3-5 students.
- Let students know that they want `large-sample` (on the worksheet) to be its own unique table. To produce a new table using `Sampler`, reopen the plugin rather than simply modifying the number of items.
- Have students complete @printable-exercise{pages/sampling-and-inference.adoc}, sharing their results and discussing with the group.
}

=== Common Misconceptions
Many people mistakenly believe that larger populations need to be represented by larger samples. In fact, the formulas that Data Scientists use to assess how good a job the sample does is only based on the _sample size_, not the population size.

[.strategy-box, cols="1", grid="none", stripes="none"]
|===
|
@span{.title}{Extension}

In a statistics-focused class, or if appropriate for your learning goals, this is a great place to include more rigorous statistics content on @link{https://www.khanacademy.org/math/ap-statistics/estimating-confidence-ap/one-sample-z-interval-proportion/v/determining-sample-size-based-on-confidence-and-margin-of-error, sample size}, @link{https://www.youtube.com/watch?v=SRwMfEmKx3A, sampling bias}, etc.
|===

=== Synthesize
Have students share.

[.lesson-instruction]
* Were larger samples always better for guessing the truth about the whole population? If so, how much better?
* Why is taking a random sample important for avoiding bias in our analyses?

[.strategy-box, cols="1", grid="none", stripes="none"]
|===
|
@span{.title}{Project Options: Food Habits / Time Use}


@opt-project{food-habits-project.adoc, rubric-food-habits.adoc} and @opt-project{time-use-project.adoc, rubric-time-use.adoc} are both projects in which students gather data about their own lives and use what they've learned in the class so far to analyze it. These projects can be used as a mid-term or formative assessment, or as a capstone for a limited implementation of Bootstrap:Data Science. Both projects also require that students break down tasks and follow a timeline - either individually or in groups. Rubrics for assessing the projects are linked in the materials section at the top of the lesson.

@span{.center}{__(Based on the projects of the same name from @link{https://www.introdatascience.org/, IDS at UCLA})__}
|===
